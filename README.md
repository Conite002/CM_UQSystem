# ğŸ§  Uncertainty Quantification on MNIST

This project implements and compares **uncertainty quantification methods** in deep neural networks using the **MNIST dataset**. The project includes:

âœ… **Baseline CNN** for classification.  
âœ… **Bayesian Neural Networks (MC Dropout)**.  
âœ… **Deep Ensembles**.  
âœ… **Evidential Deep Learning** (predicting Dirichlet parameters).  

---

## ğŸ“ **Project Structure**

project/ 
â”œâ”€â”€ data/ 
# Data processing scripts 
â”‚
â”œâ”€â”€ init.py 
â”‚ 
â””â”€â”€ load_data.py 
â”œâ”€â”€ models/ 
# Model architectures â”‚ 
â”œâ”€â”€ init.py â”‚ 
â”œâ”€â”€ baseline_cnn.py â”‚ 
â”œâ”€â”€ mc_dropout.py â”‚ 
â”œâ”€â”€ deep_ensemble.py â”‚ 
â””â”€â”€ evidential.py 
â”œâ”€â”€ utils/ # Utility functions (metrics, plots) â”‚ 
â”œâ”€â”€ init.py â”‚ â”œâ”€â”€ metrics.py â”‚ 
â””â”€â”€ visualization.py 
â”œâ”€â”€ experiments/ 
# Training scripts â”‚ 
â”œâ”€â”€ train_baseline.ipynb â”‚ 
â”œâ”€â”€ train_mc_dropout.ipynb â”‚ 
â”œâ”€â”€ train_deep_ensemble.ipynb â”‚ 
â””â”€â”€ train_evidential.ipynb 
â”œâ”€â”€ notebooks/ 
# Analysis and reports â”‚ 
â”œâ”€â”€ Data_Exploration.ipynb â”‚ 
â””â”€â”€ Uncertainty_Analysis.ipynb 
â”œâ”€â”€ requirements.txt # Dependencies 
â”œâ”€â”€ README.md # Project documentation 
â””â”€â”€ .gitignore # Files to ignore

# ğŸ— Project Workflow

ğŸ”¹ **Data Preparation**
Load MNIST dataset using data/load_data.py
Normalize images, reshape for CNN input, and split into training/testing sets.

ğŸ”¹ **Model Training**
Train the Baseline CNN, MC Dropout, Deep Ensembles, and Evidential Network using the scripts in experiments/.

ğŸ”¹ **Evaluation & Uncertainty Analysis**
Compute accuracy, predictive entropy, mutual information, and calibration metrics (ECE).
Compare uncertainty quantification across models.

ğŸ”¹ **Experiment Tracking**
(Optional) Use MLflow or Weights & Biases to track hyperparameters and results.
ğŸ“Š Model Evaluation and Calibration
âœ… Implemented Metrics:

**Accuracy**: Standard performance metric.

**Predictive Entropy**: Measures confidence in predictions.

**Mutual Information (MI)**: Estimates model uncertainty.

**Expected Calibration Error (ECE)**: Measures calibration quality.
AUROC for OOD Detection: Detects out-of-distribution samples.
# ğŸ“Œ Visualization:
Reliability Diagrams to visualize model calibration.
Uncertainty heatmaps over MNIST test images.
Comparison of uncertainty distributions (box plots, histograms).
